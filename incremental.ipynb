{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import Parallel, parallel_backend\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' \n",
    "    Some basic text cleaning phases this function should be applied to the text column\n",
    "    of the data\n",
    "    '''\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    stop = [w for w in stop if w not in ['not', 'no']]\n",
    "    text = [x for x in text if (x not in stop)]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    # tokenize the text\n",
    "    #text = text.split()\n",
    "    return(text)\n",
    "def apply_cleaning(X_train):\n",
    "    '''\n",
    "    With this fonction we apply the clean_text function to the text column of the data\n",
    "    and it gives as output the cleaned data for each row of the text column\n",
    "    '''\n",
    "    X_train = X_train.apply(lambda x: clean_text(x))\n",
    "    return(X_train)\n",
    "\n",
    "def tfidf_featured_data(X_train):\n",
    "    ''''\n",
    "    No need to explain this function because we gonna use it in the pipeline automatically this function\n",
    "    used to add tf-idfs columns to the data but I used this to play around with the data and see the\n",
    "    results of the tfidf vectorizer\n",
    "    '''\n",
    "\n",
    "    X_train = apply_cleaning(X_train)\n",
    "    # add tf-idfs columns\n",
    "    tfidf = TfidfVectorizer(min_df = 30)\n",
    "    tfidf_result = tfidf.fit_transform(X_train).toarray()\n",
    "    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names_out())\n",
    "    tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
    "    tfidf_df.index = X_train.index\n",
    "    X_train = pd.concat([X_train, tfidf_df], axis=1)\n",
    "    X_train.drop(columns = ['sentences'], inplace = True)\n",
    "    \n",
    "    return X_train\n",
    "\n",
    "def dict2text(dicti):\n",
    "    '''\n",
    "    This function is used to convert the dictionary of parameters to a string\n",
    "    used for GridsearchCV outputs to be saved in a csv file and to be used later \n",
    "    on project.\n",
    "    '''\n",
    "    text = ''\n",
    "    for key, value in dicti.items():\n",
    "        text += str(key) + '=' + str(value) + ' '\n",
    "    return text\n",
    "\n",
    "def format_params(params_str):\n",
    "    '''\n",
    "    I captured some differences between the parameters of the GridsearchCV output\n",
    "    to standardize it and make it easy to be used later on the project.\n",
    "    '''\n",
    "    # Remove the opening and closing braces and quotes\n",
    "    params_str = params_str.replace(\"{\", \"\").replace(\"}\", \"\").replace(\"'\", \"\")\n",
    "\n",
    "    # Replace commas with spaces\n",
    "    params_str = params_str.replace(\", \", \" \")\n",
    "\n",
    "    # Replace colons with equals signs\n",
    "    params_str = params_str.replace(\":\", \"=\")\n",
    "\n",
    "    # Remove spaces around equals signs\n",
    "    params_str = params_str.replace(\"= \", \"=\").replace(\" =\", \"=\")\n",
    "\n",
    "    return params_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='labels'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.read_csv('train_data_400k_randomsample.csv')\n",
    "train[\"labels\"].groupby(train[\"labels\"]).count().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What this cell does is first subsampling the data to certain batches then doing hyperparameter tunning with GridsearchCV main reason for batching is that training all data \n",
    "# which is already huge gets more huge with tf-idf feature so the bigger the data it takes more time to train and tune the model so I decided to split the data into batches\n",
    "# then taking the mean of each batch results to get the final results of the model. However slicing the data and taking the mean value of accuracy of all batches is good idea for\n",
    "# generalization but when the sliced data is too small then hyperparameter setting for the small data is not good for the whole data so it is better to be careful about generalization. \n",
    "\n",
    "# Split the data into multiple slices\n",
    "\n",
    "train[\"sentences\"] = apply_cleaning(train[\"sentences\"])\n",
    "\n",
    "slices = [train[:100000], train[100000:200000], train[200000:300000], train[300000:400000]]\n",
    "\n",
    "# Define the pipeline with normalization\n",
    "pipeline_without_norm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', SVC())\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune for each pipeline\n",
    "parameters_without_norm = {\n",
    "    'tfidf__max_features': [5000],\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i, slice in enumerate(slices):\n",
    "    print(\"Slice: \", i+1, str(len(slice)))\n",
    "    # Split the current slice into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(slice[\"sentences\"], slice[\"labels\"], test_size=0.1)\n",
    "    \n",
    "    # Create a GridSearchCV object for each pipeline\n",
    "    grid_search_without_norm = GridSearchCV(pipeline_without_norm, parameters_without_norm, cv=3, verbose=3)\n",
    "\n",
    "    with parallel_backend(backend=\"multiprocessing\", n_jobs=-1):\n",
    "        # Fit the GridSearchCV object on the training data\n",
    "        grid_search_without_norm.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels for the test set\n",
    "    y_pred = grid_search_without_norm.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy score for the current slice\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracy score and best hyperparameters for the current slice\n",
    "    print(\"Accuracy score: \", score)\n",
    "    print(\"Best hyperparameters: \", grid_search_without_norm.best_params_)\n",
    "    print(\"Best accuracy score: \", grid_search_without_norm.best_score_)\n",
    "\n",
    "    #store it in a dataframe\n",
    "    df = pd.DataFrame(list(zip(grid_search_without_norm.cv_results_['params'], grid_search_without_norm.cv_results_['mean_test_score'])), columns=['params', 'mean_test_score of batch '+str(i)])\n",
    "    df.to_csv('grid_search_results20k_batch'+str(i)+'.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "dfs = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "dfs[\"params\"] = dfs[\"params\"].apply(lambda x: dict2text(x))\n",
    "dfs = dfs.groupby(['params']).mean().to_csv('grid_search_results400k.csv')\n",
    "pd.DataFrame(zip(dfs[\"params\"].values,dfs.mean(axis=0)),columns=['params','mean_test_score']).sort_values(by='mean_test_score',ascending=False).to_csv('grid_search_results400k_no_rbfgamma_setting_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slice:  1 100\n",
      "[CV 1/3] END svc__C=0.1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.633 total time=   0.0s[CV 2/3] END svc__C=1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.700 total time=   0.0s[CV 1/3] END svc__C=1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.633 total time=   0.0s[CV 2/3] END svc__C=0.1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.667 total time=   0.0s\n",
      "\n",
      "[CV 3/3] END svc__C=0.1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.667 total time=   0.0s\n",
      "\n",
      "\n",
      "[CV 3/3] END svc__C=1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.700 total time=   0.0s[CV 2/3] END svc__C=10, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.767 total time=   0.0s[CV 1/3] END svc__C=10, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.767 total time=   0.0s\n",
      "[CV 2/3] END svc__C=0.1, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.667 total time=   0.0s\n",
      "[CV 1/3] END svc__C=1, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.700 total time=   0.0s\n",
      "\n",
      "[CV 3/3] END svc__C=0.1, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.667 total time=   0.0s[CV 2/3] END svc__C=1, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.733 total time=   0.0s\n",
      "\n",
      "[CV 3/3] END svc__C=10, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.633 total time=   0.1sFitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "[CV 1/3] END svc__C=0.1, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.633 total time=   0.1s\n",
      "\n",
      "[CV 1/3] END svc__C=10, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.767 total time=   0.0s[CV 3/3] END svc__C=1, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.700 total time=   0.0s\n",
      "[CV 1/3] END svc__C=0.1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.633 total time=   0.0s[CV 2/3] END svc__C=10, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.767 total time=   0.0s[CV 3/3] END svc__C=10, svc__dual=True, svc__loss=squared_hinge, svc__penalty=l2;, score=0.633 total time=   0.0s\n",
      "\n",
      "\n",
      "\n",
      "[CV 2/3] END svc__C=0.1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.667 total time=   0.0s[CV 3/3] END svc__C=0.1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.667 total time=   0.0s\n",
      "\n",
      "[CV 1/3] END svc__C=1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.800 total time=   0.0s\n",
      "[CV 2/3] END svc__C=1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.700 total time=   0.0s\n",
      "[CV 3/3] END svc__C=1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.633 total time=   0.0s\n",
      "[CV 1/3] END svc__C=10, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.633 total time=   0.0s\n",
      "[CV 3/3] END svc__C=10, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.600 total time=   0.0s[CV 1/3] END svc__C=1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.700 total time=   0.0s[CV 1/3] END svc__C=0.1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.633 total time=   0.0s[CV 2/3] END svc__C=10, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l1;, score=0.567 total time=   0.0s\n",
      "\n",
      "[CV 3/3] END svc__C=0.1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.667 total time=   0.0s\n",
      "[CV 2/3] END svc__C=0.1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.667 total time=   0.0s\n",
      "\n",
      "[CV 3/3] END svc__C=1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.700 total time=   0.0s\n",
      "[CV 2/3] END svc__C=10, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.767 total time=   0.0s\n",
      "\n",
      "[CV 2/3] END svc__C=1, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.733 total time=   0.0s[CV 3/3] END svc__C=10, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.633 total time=   0.0s\n",
      "\n",
      "[CV 1/3] END svc__C=10, svc__dual=False, svc__loss=squared_hinge, svc__penalty=l2;, score=0.767 total time=   0.0s\n",
      "Accuracy score:  0.5\n",
      "Best hyperparameters:  {'svc__C': 10, 'svc__dual': True, 'svc__loss': 'hinge', 'svc__penalty': 'l2'}\n",
      "Best accuracy score:  0.7222222222222223\n",
      "Slice:  2 100000\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 1/3] END svc__C=0.1, svc__dual=True, svc__loss=hinge, svc__penalty=l2;, score=0.883 total time=   7.8s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ege/datamining1/Amazon Reviews/incremental.ipynb Cell 5\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ege/datamining1/Amazon%20Reviews/incremental.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline_without_norm, filtered_params, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ege/datamining1/Amazon%20Reviews/incremental.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mwith\u001b[39;00m parallel_backend(backend\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmultiprocessing\u001b[39m\u001b[39m\"\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ege/datamining1/Amazon%20Reviews/incremental.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39m# Fit the GridSearchCV object on the training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ege/datamining1/Amazon%20Reviews/incremental.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ege/datamining1/Amazon%20Reviews/incremental.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Predict the labels for the test set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/ege/datamining1/Amazon%20Reviews/incremental.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m y_pred \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/site-packages/sklearn/model_selection/_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    819\u001b[0m         )\n\u001b[1;32m    820\u001b[0m     )\n\u001b[0;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    824\u001b[0m         clone(base_estimator),\n\u001b[1;32m    825\u001b[0m         X,\n\u001b[1;32m    826\u001b[0m         y,\n\u001b[1;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    833\u001b[0m     )\n\u001b[1;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    836\u001b[0m     )\n\u001b[1;32m    837\u001b[0m )\n\u001b[1;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    844\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/dataminingv/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# What this cell does is first subsampling the data to certain batches then doing hyperparameter tunning with GridsearchCV main reason for batching is that training all data \n",
    "# which is already huge gets more huge with tf-idf feature so the bigger the data it takes more time to train and tune the model so I decided to split the data into batches\n",
    "# then taking the mean of each batch results to get the final results of the model. However slicing the data and taking the mean value of accuracy of all batches is good idea for\n",
    "# generalization but when the sliced data is too small then hyperparameter setting for the small data is not good for the whole data so it is better to be careful about generalization. \n",
    "\n",
    "# Split the data into multiple slices\n",
    "\n",
    "# train[\"sentences\"] = apply_cleaning(train[\"sentences\"])\n",
    "\n",
    "slices = [train[:100], train[100000:200000], train[200000:300000], train[300000:400000]]\n",
    "\n",
    "# Define the pipeline with normalization\n",
    "pipeline_without_norm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', LinearSVC())\n",
    "])\n",
    "\n",
    "duals = [True, False]\n",
    "penaltys = ['l1', 'l2']\n",
    "losses = ['hinge', 'squared_hinge']\n",
    "all_params = list(product(duals, penaltys, losses))\n",
    "filtered_params = [{'dual': [dual], 'penalty' : [penalty], 'loss': [loss]}\n",
    "                   for dual, penalty, loss in all_params\n",
    "                   if not (penalty == 'l1' and loss == 'hinge') \n",
    "                   and not ((penalty == 'l1' and loss == 'squared_hinge' and dual is True))\n",
    "                  and not ((penalty == 'l2' and loss == 'hinge' and dual is False))]\n",
    "\n",
    "# Define the hyperparameters to tune for each pipeline\n",
    "c=[0.1, 1, 10]\n",
    "duals = [True, False]\n",
    "penaltys = ['l1', 'l2']\n",
    "losses = ['hinge', 'squared_hinge']\n",
    "all_params = list(product(duals, penaltys, losses,c))\n",
    "filtered_params = [{'svc__dual': [dual], 'svc__penalty' : [penalty], 'svc__loss': [loss], 'svc__C':[C]}\n",
    "                   for dual, penalty, loss, C in all_params\n",
    "                   if not (penalty == 'l1' and loss == 'hinge') \n",
    "                   and not ((penalty == 'l1' and loss == 'squared_hinge' and dual is True))\n",
    "                   and not ((penalty == 'l2' and loss == 'hinge' and dual is False))]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i, slice in enumerate(slices):\n",
    "    print(\"Slice: \", i+1, str(len(slice)))\n",
    "    # Split the current slice into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(slice[\"sentences\"], slice[\"labels\"], test_size=0.1)\n",
    "    \n",
    "    # Create a GridSearchCV object for each pipeline\n",
    "    grid_search = GridSearchCV(pipeline_without_norm, filtered_params, cv=3, verbose=3)\n",
    "\n",
    "    with parallel_backend(backend=\"multiprocessing\", n_jobs=-1):\n",
    "        # Fit the GridSearchCV object on the training data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the labels for the test set\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy score for the current slice\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Print the accuracy score and best hyperparameters for the current slice\n",
    "    print(\"Accuracy score: \", score)\n",
    "    print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "    print(\"Best accuracy score: \", grid_search.best_score_)\n",
    "\n",
    "    #store it in a dataframe\n",
    "    df = pd.DataFrame(list(zip(grid_search.cv_results_['params'], grid_search.cv_results_['mean_test_score'])), columns=['params', 'mean_test_score of batch '+str(i)])\n",
    "    df.to_csv('grid_search_results20k_batch'+str(i)+'.csv')\n",
    "    dfs.append(df)\n",
    "\n",
    "dfs = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "dfs[\"params\"] = dfs[\"params\"].apply(lambda x: dict2text(x))\n",
    "dfs = dfs.groupby(['params']).mean().to_csv('grid_search_results400k.csv')\n",
    "pd.DataFrame(zip(dfs[\"params\"].values,dfs.mean(axis=0)),columns=['params','mean_test_score']).sort_values(by='mean_test_score',ascending=False).to_csv('grid_search_results400k_no_rbfgamma_setting_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'svc__dual': [True],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['hinge'],\n",
       "  'svc__C': [0.1]},\n",
       " {'svc__dual': [True],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['hinge'],\n",
       "  'svc__C': [1]},\n",
       " {'svc__dual': [True],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['hinge'],\n",
       "  'svc__C': [10]},\n",
       " {'svc__dual': [True],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [0.1]},\n",
       " {'svc__dual': [True],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [1]},\n",
       " {'svc__dual': [True],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [10]},\n",
       " {'svc__dual': [False],\n",
       "  'svc__penalty': ['l1'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [0.1]},\n",
       " {'svc__dual': [False],\n",
       "  'svc__penalty': ['l1'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [1]},\n",
       " {'svc__dual': [False],\n",
       "  'svc__penalty': ['l1'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [10]},\n",
       " {'svc__dual': [False],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [0.1]},\n",
       " {'svc__dual': [False],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [1]},\n",
       " {'svc__dual': [False],\n",
       "  'svc__penalty': ['l2'],\n",
       "  'svc__loss': ['squared_hinge'],\n",
       "  'svc__C': [10]}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=[0.1, 1, 10]\n",
    "duals = [True, False]\n",
    "penaltys = ['l1', 'l2']\n",
    "losses = ['hinge', 'squared_hinge']\n",
    "all_params = list(product(duals, penaltys, losses,c))\n",
    "filtered_params = [{'svc__dual': [dual], 'svc__penalty' : [penalty], 'svc__loss': [loss], 'svc__C':[C]}\n",
    "                   for dual, penalty, loss, C in all_params\n",
    "                   if not (penalty == 'l1' and loss == 'hinge') \n",
    "                   and not ((penalty == 'l1' and loss == 'squared_hinge' and dual is True))\n",
    "                   and not ((penalty == 'l2' and loss == 'hinge' and dual is False))]\n",
    "\n",
    "filtered_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(True, 'l1', 'hinge', 0.1),\n",
       " (True, 'l1', 'hinge', 1),\n",
       " (True, 'l1', 'hinge', 10),\n",
       " (True, 'l1', 'squared_hinge', 0.1),\n",
       " (True, 'l1', 'squared_hinge', 1),\n",
       " (True, 'l1', 'squared_hinge', 10),\n",
       " (True, 'l2', 'hinge', 0.1),\n",
       " (True, 'l2', 'hinge', 1),\n",
       " (True, 'l2', 'hinge', 10),\n",
       " (True, 'l2', 'squared_hinge', 0.1),\n",
       " (True, 'l2', 'squared_hinge', 1),\n",
       " (True, 'l2', 'squared_hinge', 10),\n",
       " (False, 'l1', 'hinge', 0.1),\n",
       " (False, 'l1', 'hinge', 1),\n",
       " (False, 'l1', 'hinge', 10),\n",
       " (False, 'l1', 'squared_hinge', 0.1),\n",
       " (False, 'l1', 'squared_hinge', 1),\n",
       " (False, 'l1', 'squared_hinge', 10),\n",
       " (False, 'l2', 'hinge', 0.1),\n",
       " (False, 'l2', 'hinge', 1),\n",
       " (False, 'l2', 'hinge', 10),\n",
       " (False, 'l2', 'squared_hinge', 0.1),\n",
       " (False, 'l2', 'squared_hinge', 1),\n",
       " (False, 'l2', 'squared_hinge', 10)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(product(duals, penaltys, losses,c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
